import asyncio
import logging
import os
import base64
from dataclasses import dataclass
from typing import Callable, Set, List, Tuple

import aiohttp
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from openai import AsyncOpenAI

# =========================
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
# =========================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
)
logger = logging.getLogger("news-bot")

load_dotenv()

BOT_TOKEN = os.getenv("BOT_TOKEN")
TARGET_CHAT_ID = os.getenv("TARGET_CHAT_ID")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

CHECK_INTERVAL = 60 * 2
TOTAL_PER_CYCLE = 24

if not BOT_TOKEN or not TARGET_CHAT_ID:
    raise RuntimeError("BOT_TOKEN –∏–ª–∏ TARGET_CHAT_ID –Ω–µ –∑–∞–¥–∞–Ω—ã")

if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω")

TELEGRAM_API = f"https://api.telegram.org/bot{BOT_TOKEN}"
openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)

# =========================
# –ò—Å—Ç–æ—á–Ω–∏–∫–∏
# =========================

@dataclass(slots=True)
class NewsSource:
    name: str
    url: str
    parser: Callable[[BeautifulSoup], List[Tuple[str, str]]]

def parse_generic(soup, selector, base=""):
    result = []
    for a in soup.select(selector):
        title = a.get_text(strip=True)
        link = a.get("href")
        if title and link:
            if link.startswith("/") and base:
                link = base + link
            result.append((title, link))
    return result

SOURCES = [
    NewsSource("Destructoid", "https://www.destructoid.com/news/",
               lambda s: parse_generic(s, "a.title", "https://www.destructoid.com")),
    NewsSource("PC Gamer", "https://www.pcgamer.com/news/",
               lambda s: parse_generic(s, "a.article-link")),
    NewsSource("Rock Paper Shotgun", "https://www.rockpapershotgun.com/news/",
               lambda s: parse_generic(s, "a.c-block-link__overlay")),
    NewsSource("NVIDIA Blog", "https://blogs.nvidia.com/",
               lambda s: parse_generic(s, "a.blog-card__link")),
]

sent_links: Set[str] = set()

# =========================
# Telegram
# =========================

async def send_photo_with_caption(session, image_bytes, caption):
    try:
        data = aiohttp.FormData()
        data.add_field("chat_id", TARGET_CHAT_ID)
        data.add_field("photo", image_bytes,
                       filename="digest.png",
                       content_type="image/png")
        data.add_field("caption", caption)
        data.add_field("parse_mode", "Markdown")

        async with session.post(
            f"{TELEGRAM_API}/sendPhoto",
            data=data,
            timeout=aiohttp.ClientTimeout(total=30)
        ) as r:
            text = await r.text()
            if r.status != 200:
                logger.error(f"Telegram API error {r.status}: {text}")
            else:
                logger.info("Post successfully sent to Telegram")

    except Exception:
        logger.exception("Failed to send message to Telegram")

# =========================
# OpenAI helper
# =========================

async def ask_gpt(messages, temperature=0.6):
    try:
        response = await openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=temperature,
        )
        return response.choices[0].message.content.strip()
    except Exception:
        logger.exception("OpenAI request failed")
        raise

# =========================
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
# =========================

async def generate_digest(news_items):
    formatted = "\n".join(
        f"- [{src}] {title} ({link})"
        for src, title, link in news_items
    )

    chatgpt_prompt = (
        "–°–æ—Å—Ç–∞–≤—å –¥–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –∏–≥—Ä–æ–≤–æ–≥–æ –∏ IT-–º–∏—Ä–æ–≤, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
        "–ö–∞–∂–¥–∞—è –Ω–æ–≤–æ—Å—Ç—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –Ω–µ –º–µ–Ω–µ–µ 250 —Å–∏–º–≤–æ–ª–æ–≤. "
        "–ù–æ–≤–æ—Å—Ç–∏ –¥–æ–ª–∂–Ω—ã —Ä–∞–∑–¥–µ–ª—è—Ç—å—Å—è –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π. "
        "–í –Ω–∞—á–∞–ª–µ –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø–æ —Ç–µ–º–∞—Ç–∏–∫–µ —Å—Ç–∏–∫–µ—Ä (–æ–¥–∏–Ω —ç–º–æ–¥–∑–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é, "
        "–Ω–∞–ø—Ä–∏–º–µ—Ä üéÆ –¥–ª—è –∏–≥—Ä, üß† –¥–ª—è –ò–ò, üñ• –¥–ª—è –∂–µ–ª–µ–∑–∞, üîí –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, üöÄ –¥–ª—è –∑–∞–ø—É—Å–∫–æ–≤, üíæ –¥–ª—è –ü–û, üïπ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π). "
        "–ë–µ–∑ –Ω—É–º–µ—Ä–∞—Ü–∏–∏, –±–µ–∑ —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫.\n\n"
        f"{formatted}"
    )

    return await ask_gpt(messages)

async def generate_image(digest_text):
    try:
        response = await openai_client.images.generate(
            model="gpt-image-1",
            prompt=digest_text[:800],
            size="1024x1024",
        )
        image_base64 = response.data[0].b64_json
        return base64.b64decode(image_base64)
    except Exception:
        logger.exception("Image generation failed")
        raise

# =========================
# –ü–∞—Ä—Å–∏–Ω–≥
# =========================

async def fetch_html(session, url):
    try:
        async with session.get(
            url,
            headers={"User-Agent": "Mozilla/5.0"},
            timeout=aiohttp.ClientTimeout(total=30)
        ) as r:
            return await r.text()
    except Exception:
        logger.exception(f"Failed to fetch {url}")
        raise

async def parse_sources(session):
    collected = []
    for source in SOURCES:
        try:
            html = await fetch_html(session, source.url)
            soup = BeautifulSoup(html, "html.parser")
            articles = source.parser(soup)

            for title, link in articles:
                if link not in sent_links:
                    collected.append((source.name, title, link))

        except Exception:
            logger.exception(f"Error while parsing {source.name}")

    return collected[:TOTAL_PER_CYCLE]

# =========================
# –û—á–µ—Ä–µ–¥–∏
# =========================

news_queue: asyncio.Queue = asyncio.Queue()
publish_queue: asyncio.Queue = asyncio.Queue()

# =========================
# Workers
# =========================

async def parser_worker():
    logger.info("Parser worker started")
    while True:
        try:
            async with aiohttp.ClientSession() as session:
                news = await parse_sources(session)
                if news:
                    await news_queue.put(news)
                    logger.info(f"Collected {len(news)} new articles")

        except asyncio.CancelledError:
            logger.warning("Parser worker cancelled")
            raise
        except Exception:
            logger.exception("Unexpected error in parser_worker")

        await asyncio.sleep(CHECK_INTERVAL)

async def generator_worker():
    logger.info("Generator worker started")
    while True:
        try:
            news_items = await news_queue.get()

            digest = await generate_digest(news_items)
            image = await generate_image(digest)

            await publish_queue.put((digest, image, news_items))
            news_queue.task_done()

        except asyncio.CancelledError:
            logger.warning("Generator worker cancelled")
            raise
        except Exception:
            logger.exception("Unexpected error in generator_worker")

async def publisher_worker():
    logger.info("Publisher worker started")
    while True:
        try:
            digest, image, news_items = await publish_queue.get()

            async with aiohttp.ClientSession() as session:
                await send_photo_with_caption(session, image, digest)

            for _, _, link in news_items:
                sent_links.add(link)

            publish_queue.task_done()

        except asyncio.CancelledError:
            logger.warning("Publisher worker cancelled")
            raise
        except Exception:
            logger.exception("Unexpected error in publisher_worker")

# =========================
# Heartbeat
# =========================

async def heartbeat():
    while True:
        logger.info("Bot is alive")
        await asyncio.sleep(300)

# =========================
# Main
# =========================

async def main():
    loop = asyncio.get_running_loop()

    def handle_async_exception(loop, context):
        logger.error(f"Unhandled asyncio exception: {context}", exc_info=True)

    loop.set_exception_handler(handle_async_exception)

    tasks = [
        asyncio.create_task(parser_worker()),
        asyncio.create_task(generator_worker()),
        asyncio.create_task(publisher_worker()),
        asyncio.create_task(heartbeat())
    ]

    try:
        await asyncio.gather(*tasks)
    except Exception:
        logger.exception("Fatal error in main()")
    finally:
        for task in tasks:
            task.cancel()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception:
        logger.exception("Bot crashed at top level")



